{"cells":[{"cell_type":"markdown","metadata":{"id":"eHKvB2vUL3ow"},"source":["Semantic Segmentation \n","============\n","\n","In this exercise you are going to work on a computer vision task called semantic segmentation. In comparison to image classification the goal is not to classify an entire image but each of its pixels separately. This implies that the  output of the network is not a single scalar but a segmentation with the same shape as the input image. Think about why you should rather use convolutional than fully-connected layers for this task!\n","\n","<img src=\"https://camo.githubusercontent.com/d10b897e15344334e449104a824aff6c29125dc2/687474703a2f2f63616c76696e2e696e662e65642e61632e756b2f77702d636f6e74656e742f75706c6f6164732f646174612f636f636f7374756666646174617365742f636f636f73747566662d6578616d706c65732e706e67\">"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"04zmpgXaMp4v"},"outputs":[],"source":["#!pip install pytorch-lightning"]},{"cell_type":"markdown","metadata":{"id":"YZVzl2e6L3o0"},"source":["# 1. Preparation\n","\n","## Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZWkotx-kL3o0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import torch\n","\n","from exercise_code.data.segmentation_dataset import SegmentationData, label_img_to_rgb\n","from exercise_code.data.download_utils import download_dataset\n","from exercise_code.util import visualizer, save_model\n","from exercise_code.util.Util import checkSize, checkParams, test\n","from exercise_code.networks.segmentation_nn import SegmentationNN, DummySegmentationModel\n","from exercise_code.tests import test_seg_nn\n","#set up default cuda device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"_UQkr7_kL3o2"},"source":["## Load and Visualize Data\n","\n","#### MSRC-v2 Segmentation Dataset\n","\n","The MSRC v2 dataset is an extension of the MSRC v1 dataset from Microsoft Research in Cambridge. It contains *591* images and *23* object classes with accurate pixel-wise labeled images. \n","\n","\n","\n","The image ids are stored in the txt file `train.txt`, `val.txt`, `test.txt`. The dataloader will read the image id in the txt file and fetch the corresponding input and target images from the image folder. \n","<img src='images/input_target.png'/>\n","\n","\n","\n","As you can see in `exercise_code/data/segmentation_dataset.py`, each segmentation label has its corresponding RGB value stored in the `SEG_LABELS_LIST`. The label `void` means `unlabeled`, and it is displayed as black `\"rgb_values\": [0, 0, 0]` in the target image. The target image pixels will be labeled based on its color using `SEG_LABELS_LIST`.\n","\n","```python\n","                SEG_LABELS_LIST = [\n","                {\"id\": -1, \"name\": \"void\",       \"rgb_values\": [0,   0,    0]},\n","                {\"id\": 0,  \"name\": \"building\",   \"rgb_values\": [128, 0,    0]},\n","                {\"id\": 1,  \"name\": \"grass\",      \"rgb_values\": [0,   128,  0]},\n","                {\"id\": 2,  \"name\": \"tree\",       \"rgb_values\": [128, 128,  0]},\n","                {\"id\": 3,  \"name\": \"cow\",        \"rgb_values\": [0,   0,    128]},\n","                {\"id\": 4,  \"name\": \"horse\",      \"rgb_values\": [128, 0,    128]},\n","                {\"id\": 5,  \"name\": \"sheep\",      \"rgb_values\": [0,   128,  128]},\n","                ...]    \n","```"]},{"cell_type":"markdown","metadata":{"id":"vIdqn_fzL3o2","pycharm":{"name":"#%% md\n"}},"source":["<div class=\"alert alert-block alert-warning\">\n","    <h3>Note: The label <code>void</code></h3>\n","    <p>Pixels with the label <code>void</code> should neither be considered in your loss nor in the accuracy of your segmentation. See implementation for details.</p>\n","</div>"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"z5AkG9mDL3o3"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/jamesqian/Documents/DeepVision/datasets/segmentation/segmentation_data/val.txt'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=5'>6</a>\u001b[0m download_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=6'>7</a>\u001b[0m     url\u001b[39m=\u001b[39mdownload_url,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=7'>8</a>\u001b[0m     data_dir\u001b[39m=\u001b[39mdata_root,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=8'>9</a>\u001b[0m     dataset_zip_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msegmentation_data.zip\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=9'>10</a>\u001b[0m     force_download\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=12'>13</a>\u001b[0m train_data \u001b[39m=\u001b[39m SegmentationData(image_paths_file\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdata_root\u001b[39m}\u001b[39;00m\u001b[39m/segmentation_data/train.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=13'>14</a>\u001b[0m val_data \u001b[39m=\u001b[39m SegmentationData(image_paths_file\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_root\u001b[39m}\u001b[39;49;00m\u001b[39m/segmentation_data/val.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000006?line=14'>15</a>\u001b[0m test_data \u001b[39m=\u001b[39m SegmentationData(image_paths_file\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdata_root\u001b[39m}\u001b[39;00m\u001b[39m/segmentation_data/test.txt\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/Documents/DeepVision/HW10/exercise_code/data/segmentation_dataset.py:60\u001b[0m, in \u001b[0;36mSegmentationData.__init__\u001b[0;34m(self, image_paths_file)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jamesqian/Documents/DeepVision/HW10/exercise_code/data/segmentation_dataset.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, image_paths_file):\n\u001b[1;32m     <a href='file:///home/jamesqian/Documents/DeepVision/HW10/exercise_code/data/segmentation_dataset.py?line=57'>58</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_dir_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(image_paths_file)\n\u001b[0;32m---> <a href='file:///home/jamesqian/Documents/DeepVision/HW10/exercise_code/data/segmentation_dataset.py?line=59'>60</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(image_paths_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='file:///home/jamesqian/Documents/DeepVision/HW10/exercise_code/data/segmentation_dataset.py?line=60'>61</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_names \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39msplitlines()\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jamesqian/Documents/DeepVision/datasets/segmentation/segmentation_data/val.txt'"]}],"source":["download_url = 'http://i2dl.vc.in.tum.de/static/data/segmentation_data.zip'\n","i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n","data_root = os.path.join(i2dl_exercises_path, 'datasets','segmentation')\n","\n","\n","download_dataset(\n","    url=download_url,\n","    data_dir=data_root,\n","    dataset_zip_name='segmentation_data.zip',\n","    force_download=False,\n",")\n","\n","train_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/train.txt')\n","val_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/val.txt')\n","test_data = SegmentationData(image_paths_file=f'{data_root}/segmentation_data/test.txt')"]},{"cell_type":"markdown","metadata":{"id":"vdo1NV4BL3o4"},"source":["If you want to implement data augmentation methods, make yourself familiar with the segmentation dataset and how we implemented the `SegmentationData` class in `exercise_code/data/segmentation_dataset.py`. Furthermore, you can check the original label description in `datasets/segmentation/segmentation_data/info.html`.\n","\n","For now, let's look at a few samples of our training set:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"krdG9rHHL3o4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 2\n"]},{"ename":"NameError","evalue":"name 'val_data' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain size: \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(train_data))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000008?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mValidation size: \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(val_data))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000008?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mImg size: \u001b[39m\u001b[39m\"\u001b[39m, train_data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jamesqian/Documents/DeepVision/HW10/1_segmentation_nn.ipynb#ch0000008?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSegmentation size: \u001b[39m\u001b[39m\"\u001b[39m, train_data[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msize())\n","\u001b[0;31mNameError\u001b[0m: name 'val_data' is not defined"]}],"source":["print(\"Train size: %i\" % len(train_data))\n","print(\"Validation size: %i\" % len(val_data))\n","print(\"Img size: \", train_data[0][0].size())\n","print(\"Segmentation size: \", train_data[0][1].size())\n","\n","num_example_imgs = 4\n","plt.figure(figsize=(10, 5 * num_example_imgs))\n","for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n","    # img\n","    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n","    plt.imshow(img.numpy().transpose(1,2,0))\n","    plt.axis('off')\n","    if i == 0:\n","        plt.title(\"Input image\")\n","    \n","    # target\n","    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n","    plt.imshow(label_img_to_rgb(target.numpy()))\n","    plt.axis('off')\n","    if i == 0:\n","        plt.title(\"Target image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6tintH7RL3o5"},"source":["We can already see that the dataset is quite small in comparison to our previous datasets, e.g., for CIFAR10 we had ten thousands of images while we only have 276 training images in this case. In addition, the task is much more difficult than a \"simple 10 class classification\", as we have to assign a label to each pixel! What's more, the images are much bigger as we are now considering images of size 240x240 instead of 32x32. \n","\n","That means that you shouldn't expect our networks to perform very well, so don't be too disappointed."]},{"cell_type":"markdown","metadata":{"id":"ArsN7ixBL3o5","pycharm":{"name":"#%% md\n"}},"source":["# 2. Semantic Segmentation \n","\n","## Dummy Model\n","\n","In `exercise_code/networks/segmentation_nn.py` we define a naive `DummySegmentationModel`, which always predicts the scores of segmentation labels of the first image. Let's try it on a few images and visualize the outputs using the `visualizer` we provide. The `visualizer` takes in the model and dataset, and visualizes the first four (Input, Target, Prediction) pairs. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVU6tvlWL3o6","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["dummy_model = DummySegmentationModel(target_image=train_data[0][1])\n","\n","# Visualization function\n","visualizer(dummy_model, train_data)"]},{"cell_type":"markdown","metadata":{"id":"QL3fVMBEL3o6","pycharm":{"name":"#%% md\n"}},"source":["You can use the visualizer function in your training scenario to print out your model predictions on a regular basis."]},{"cell_type":"markdown","metadata":{"id":"s1us-l3ZL3o6"},"source":["## Loss and Metrics\n","The loss function for the task of image segmentation is a pixel-wise cross entropy loss. This loss examines each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector. \n","<img src='images/loss_img.png' width=80% height=80%/>\n","source: https://www.jeremyjordan.me/semantic-segmentation/\n","\n","Up until now we only used the default loss function (`nn.CrossEntropyLoss`) in our solvers. However, In order to ignore the `unlabeled` pixels for the computation of our loss, we have to use a customized version of the loss for the initialization of our segmentation solver. The `ignore_index` argument of the loss can be used to filter the `unlabeled` pixels and computes the loss only over remaining pixels.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NO9w1hsuL3o7","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["loss_func = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n","\n","for (inputs, targets) in train_data[0:4]:\n","    inputs, targets = inputs, targets\n","    outputs = dummy_model(inputs.unsqueeze(0))\n","    losses = loss_func(outputs, targets.unsqueeze(0))\n","    print(losses)"]},{"cell_type":"markdown","metadata":{"id":"0b-G0-QZL3o7"},"source":["<div class=\"alert alert-warning\">\n","    <h3>Note: Non-zero loss for the first sample</h3> \n","    <p>The output of our dummy model is one-hot-coded tensor. Since there is <b>softmax</b> function in the <b>nn.CrossEntropyLoss</b> function, the loss is:  \n","    $$loss(x, class) = - \\log \\left( \\frac{\\exp(x[class])}{\\Sigma_j \\exp (x[j])} \\right) = âˆ’x[class]+\\log \\left( \\Sigma_j \\exp(x[j]) \\right)$$\n","     and the loss will not be zero.    </p>\n","<p>i.e. for $x=[0, 0, 0, 1],class=3$,$\\quad$ the loss: \n","$loss(x,class) = -1 +\\log(\\exp(0)+\\exp(0)+\\exp(0)+\\exp(1)) = 0.7437$ </p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"ghXOele0L3o7","pycharm":{"name":"#%% md\n"}},"source":["To obtain an evaluation accuracy, we can simply compute the average per pixel accuracy of our network for a given image. We will use the following function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbrtUcQYL3o7","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["def evaluate_model(model, dataloader):\n","    test_scores = []\n","    model.eval()\n","    for inputs, targets in dataloader:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        outputs = model.forward(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        targets_mask = targets >= 0\n","        test_scores.append(np.mean((preds.cpu() == targets.cpu())[targets_mask].numpy()))\n","\n","    return np.mean(test_scores)\n","\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=1,shuffle=False,num_workers=0)\n","print(evaluate_model(dummy_model, test_loader))"]},{"cell_type":"markdown","metadata":{"id":"6b4qnA0pL3o8","pycharm":{"name":"#%% md\n"}},"source":["You will see reasonably high numbers as your accuracy when you do the training later. The reason behind that is the fact that most output pixels are of a single class and the network can just overfit to common classes such as \"grass\"."]},{"cell_type":"markdown","metadata":{"id":"a_7wvrXcL3o8","pycharm":{"name":"#%% md\n"}},"source":["## Step 1: Design your own model\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p>Implement your network architecture in <code>exercise_code/networks/segmentation_nn.py</code>. In this task, you will use pytorch to setup your model.\n","    </p>\n","</div>\n","\n","To compensate for the dimension reduction of a typical convolution layer, you should probably include either a single `nn.Upsample` layer, use a combination of upsampling layers as well as convolutions or even transposed convolutions near the end of your network to get back to the target image shape.\n","\n","This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). \n","The only rules your model design has to follow are:\n","* Inherit from `torch.nn.Module` or `pytorch_lightning.LightningModule`\n","* Perform the forward pass in `forward()`. Input dimension is (N, C, H, W) and output dimension is (N, num_classes, H, W)\n","* Have less than 5 million parameters\n","* Have a model size of less than 50MB after saving\n","\n","Furthermore, you need to pass all your hyperparameters to the model in a single dict `hparams`."]},{"cell_type":"markdown","metadata":{"id":"OmSDOG0GL3o8"},"source":["<div class=\"alert alert-warning\">\n","    <h3>Note: Transfer learning</h3>\n","    <p>In this exercise, we encourage you to do transfer learning as we learned in exercise 8, since this will boost your model performance and save training time. You can import pretrained models from torchvision in your model and use its feature extractor (e.g. <code>alexnet.features</code>) to get the image feature. Feel free to choose more advanced pretrained model like ResNet, MobileNet for your architecture design.</p>       \n","</div>\n","\n","See [here](https://pytorch.org/vision/stable/models.html) for more info of the torchvison pretrained models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XoGT0y7L3o8","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["hparams = {\n","    # TODO: if you have any model arguments/hparams, define them here and read them from this dict inside SegmentationNN class\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmEehFX_L3o9"},"outputs":[],"source":["model = SegmentationNN(hparams = hparams)\n","test_seg_nn(model)"]},{"cell_type":"markdown","metadata":{"id":"ssdSHgM1L3o9"},"source":["## Step 2: Train your own model\n","\n","<div class=\"alert alert-info\">\n","    <h3>Task: Implement</h3>\n","    <p> In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n","    </p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVqebG0pL3o9","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["model = SegmentationNN(hparams=hparams)\n","########################################################################\n","# TODO - Train Your Model                                              #\n","########################################################################\n","\n","\n","#######################################################################\n","#                           END OF YOUR CODE                          #\n","#######################################################################"]},{"cell_type":"markdown","metadata":{"id":"UMLbUPU2L3o9","pycharm":{"name":"#%% md\n"}},"source":["# 3. Test your Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"usFlIFuEL3o9"},"outputs":[],"source":["test(evaluate_model(model, test_loader))"]},{"cell_type":"markdown","metadata":{"id":"y7ANSfLjL3o-"},"source":["# 4. Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8TtH6E9L3o-","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["visualizer(model, test_data)"]},{"cell_type":"markdown","metadata":{"id":"oDdgQ2BqL3o-","pycharm":{"name":"#%% md\n"}},"source":["## Save the Model for Submission\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj5YxkTKL3o-","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["os.makedirs('models', exist_ok=True)\n","save_model(model, \"segmentation_nn.model\")\n","checkSize(path = \"./models/segmentation_nn.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsqmpCwpL3o-"},"outputs":[],"source":["from exercise_code.util.submit import submit_exercise\n","\n","submit_exercise('exercise10')"]},{"cell_type":"markdown","metadata":{"id":"2MEyS33JL3o-","pycharm":{"name":"#%% md\n"}},"source":["\n","# Submission Goals\n","\n","- Goal: Implement and train a convolutional neural network for Semantic Segmentation.\n","- Passing Criteria: Reach **Accuracy >= 64%** on __our__ test dataset. The submission system will show you your score after you submit.\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"1_segmentation_nn.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
